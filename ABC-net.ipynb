{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31fb467e373041e6a3a0c68bc6dc4182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_582b5a7c39544f129763cc4be3a37e33",
              "IPY_MODEL_11ab7f5e9970405bb32597029a506e99",
              "IPY_MODEL_6259fe3404a042199a887acb0449d8f8"
            ],
            "layout": "IPY_MODEL_7fd6f9343bd34175b04dc94fb95a8c3f"
          }
        },
        "582b5a7c39544f129763cc4be3a37e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_529647bb4cb347168de8f8c187285da8",
            "placeholder": "​",
            "style": "IPY_MODEL_7f891d4a5e9f41d4bb8d1d5a88ff5b5c",
            "value": ""
          }
        },
        "11ab7f5e9970405bb32597029a506e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_811aa04ee6fa4c56a3002fea6786ceb0",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fdb124403494b10afef399957de0e93",
            "value": 170498071
          }
        },
        "6259fe3404a042199a887acb0449d8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6da1aba2ebf40df96e651f3d5024215",
            "placeholder": "​",
            "style": "IPY_MODEL_0820bde0a2954010b67c24af8e8bc0cc",
            "value": " 170499072/? [00:02&lt;00:00, 64213822.51it/s]"
          }
        },
        "7fd6f9343bd34175b04dc94fb95a8c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529647bb4cb347168de8f8c187285da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f891d4a5e9f41d4bb8d1d5a88ff5b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "811aa04ee6fa4c56a3002fea6786ceb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fdb124403494b10afef399957de0e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6da1aba2ebf40df96e651f3d5024215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0820bde0a2954010b67c24af8e8bc0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF2GJKYmC1L_"
      },
      "source": [
        "# Implementation of Accurate Binary Convolution Layer\n",
        "[Original Paper](https://arxiv.org/abs/1711.11294)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQfBw7QtC1MI"
      },
      "source": [
        "The inspiration for this network is the use of Deep Neural Networks for real-time object recognition. Currently available **Convolution Layers** require large amount of computation power at runtime and that hinders the use of very deep networks in embedded systems or ASICs. Xiaofan Lin, Cong Zhao, and Wei Pan presented a way to convert Convolution Layers to **Binary Convolution Layers** for faster realtime computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-e8bqxbRKk6"
      },
      "source": [
        "The inspiration for this network is the use of Deep Neural Networks for real-time object recognition. Currently available **Convolution Layers** require large amount of computation power at runtime and that hinders the use of very deep networks in embedded systems or ASICs. Xiaofan Lin, Cong Zhao, and Wei Pan presented a way to convert Convolution Layers to **Binary Convolution Layers** for faster realtime computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvQQb1uQRMMB"
      },
      "source": [
        "The inspiration for this network is the use of Deep Neural Networks for real-time object recognition. Currently available **Convolution Layers** require large amount of computation power at runtime and that hinders the use of very deep networks in embedded systems or ASICs. Xiaofan Lin, Cong Zhao, and Wei Pan presented a way to convert Convolution Layers to **Binary Convolution Layers** for faster realtime computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTAboYlTC1MM"
      },
      "source": [
        "We'll need mean and standard deviation of the complete convolution filters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class ABCConv2d(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels,\n",
        "                 kernel_size=-1, stride=-1, padding=-1, groups=1, dropout=0.0,\n",
        "                 linear=False, base_number=3):\n",
        "        super(ABCConv2d, self).__init__()\n",
        "        assert base_number == 3 or base_number == 1, \"support base_number == 3 or base_number == 1 \"\n",
        "        self.layer_type = 'ABC_Conv2d'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dropout_ratio = dropout\n",
        "        self.base_number = base_number\n",
        "        if dropout != 0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = linear\n",
        "        if not self.linear:\n",
        "            self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
        "            if self.base_number == 1:\n",
        "                self.bases_conv2d_1 = nn.Conv2d(input_channels, output_channels,\n",
        "                                                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            else:\n",
        "\n",
        "                self.bases_conv2d_1 = nn.Conv2d(input_channels, output_channels,\n",
        "                                                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "                self.bases_conv2d_2 = nn.Conv2d(input_channels, output_channels,\n",
        "                                                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "                self.bases_conv2d_3 = nn.Conv2d(input_channels, output_channels,\n",
        "                                                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "\n",
        "        else:\n",
        "            self.bn = nn.BatchNorm1d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
        "            if self.base_number == 1:\n",
        "                self.bases_linear_1 = nn.Linear(input_channels, output_channels)\n",
        "            else:\n",
        "                self.bases_linear_1 = nn.Linear(input_channels, output_channels)\n",
        "                self.bases_linear_2 = nn.Linear(input_channels, output_channels)\n",
        "                self.bases_linear_3 = nn.Linear(input_channels, output_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        # x = BinActive()(x)\n",
        "        if self.dropout_ratio != 0:\n",
        "            x = self.dropout(x)\n",
        "        if self.base_number == 1:\n",
        "            if not self.linear:\n",
        "                x = self.bases_conv2d_1(x)\n",
        "            else:\n",
        "                x = self.bases_linear_1(x)\n",
        "        else:\n",
        "            if not self.linear:\n",
        "                x = self.bases_conv2d_1(x) + self.bases_conv2d_2(x) + self.bases_conv2d_3(x)\n",
        "            else:\n",
        "                x = self.bases_linear_1(x) + self.bases_linear_2(x) + self.bases_linear_3(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "BinConv2d = ABCConv2d\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10, base_number=3):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.base_number = base_number\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(64, eps=1e-4, momentum=0.1, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            BinConv2d(64, 192, kernel_size=5, stride=1, padding=2, groups=1, base_number=self.base_number),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            BinConv2d(192, 384, kernel_size=3, stride=1, padding=1, base_number=self.base_number),\n",
        "            BinConv2d(384, 256, kernel_size=3, stride=1, padding=1, groups=1, base_number=self.base_number),\n",
        "            BinConv2d(256, 256, kernel_size=3, stride=1, padding=1, groups=1, base_number=self.base_number),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            BinConv2d(256 * 6 * 6, 4096, linear=True, base_number=self.base_number),\n",
        "            BinConv2d(4096, 4096, dropout=0.1, linear=True, base_number=self.base_number),\n",
        "            nn.BatchNorm1d(4096, eps=1e-3, momentum=0.1, affine=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def my_model_loader(self, state_dict, strict=True):\n",
        "        own_state = self.state_dict()\n",
        "        # map fp model to ABC-Net\n",
        "        load_map = \\\n",
        "            {\n",
        "                'features.0.weight': 'features.0.weight',\n",
        "                'features.0.bias': 'features.0.bias',\n",
        "                'features.4.bases_conv2d_1.weight': 'features.3.weight',\n",
        "                'features.4.bases_conv2d_1.bias': 'features.3.bias',\n",
        "                'features.6.bases_conv2d_1.weight': 'features.6.weight',\n",
        "                'features.6.bases_conv2d_1.bias': 'features.6.bias',\n",
        "                'features.7.bases_conv2d_1.weight': 'features.8.weight',\n",
        "                'features.7.bases_conv2d_1.bias': 'features.8.bias',\n",
        "                'features.8.bases_conv2d_1.weight': 'features.10.weight',\n",
        "                'features.8.bases_conv2d_1.bias': 'features.10.bias',\n",
        "                'classifier.0.bases_linear_1.weight': 'classifier.1.weight',\n",
        "                'classifier.0.bases_linear_1.bias': 'classifier.1.bias',\n",
        "                'classifier.1.bases_linear_1.weight': 'classifier.4.weight',\n",
        "                'classifier.1.bases_linear_1.bias': 'classifier.4.bias',\n",
        "                'classifier.4.weight': 'classifier.6.weight',\n",
        "                'classifier.4.bias': 'classifier.6.bias',\n",
        "            }\n",
        "\n",
        "        for k, v in load_map.items():\n",
        "            own_state[k].copy_(state_dict[v].data)\n",
        "\n",
        "\n",
        "def alexnet(pretrained=False, **kwargs):\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    if pretrained:\n",
        "        model_path = 'model_list/alexnet_fp_pretrained.pth'\n",
        "        pretrained_model = torch.load(model_path)\n",
        "        model.my_model_loader(pretrained_model)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "zU2uIUZqteUo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0truaaVWC1MP"
      },
      "source": [
        "We need to spread the standard deviation by the number of filters being used as in the original paper\n",
        "$\\mu_i= -1 + (i - 1)\\frac{2}{\\mathbf{M} - 1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUIGQJuvC1MS"
      },
      "source": [
        "Now, we can get the values of $\\mathbf{B_{i}s}$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import platform\n",
        "\n",
        "\n",
        "class BinOp():\n",
        "    def __init__(self, model):\n",
        "        self.base_number = model.base_number\n",
        "        # count the number of Conv2d and Linear\n",
        "        count_targets = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                count_targets = count_targets + 1\n",
        "\n",
        "        start_range = 1\n",
        "        end_range = count_targets - 2\n",
        "        self.bin_range = numpy.linspace(start_range,\n",
        "                                        end_range, end_range - start_range + 1) \\\n",
        "            .astype('int').tolist()\n",
        "        self.num_of_params = len(self.bin_range)\n",
        "        self.saved_params = []\n",
        "        self.target_params = []\n",
        "        self.target_modules = []\n",
        "        self.alphas = []\n",
        "        index = -1\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                index = index + 1\n",
        "                if index in self.bin_range:\n",
        "                    tmp = m.weight.data.clone()\n",
        "                    self.saved_params.append(tmp)\n",
        "                    self.target_modules.append(m.weight)\n",
        "\n",
        "        for index_conv in range(int(self.num_of_params / self.base_number)):\n",
        "            self.alphas.append(torch.zeros(self.base_number))\n",
        "\n",
        "    def binarization(self):\n",
        "        # self.meancenterConvParams()\n",
        "        self.clampConvParams()\n",
        "        self.save_params()\n",
        "        self.binarizeConvParams()\n",
        "\n",
        "    def clampConvParams(self):\n",
        "        for index in range(int(self.num_of_params / self.base_number)):\n",
        "            self.target_modules[index * self.base_number].data=torch.clamp(\n",
        "                                                                     self.target_modules[\n",
        "                                                                         index * self.base_number].data,-1.0, 1.0)\n",
        "\n",
        "    def save_params(self):\n",
        "        for index in range(int(self.num_of_params / self.base_number)):\n",
        "            self.saved_params[index * self.base_number].copy_(self.target_modules[index * self.base_number].cpu().data)\n",
        "\n",
        "    def ABC_binarizeConvParams(self):\n",
        "        for index_conv in range(int(self.num_of_params / self.base_number)):\n",
        "            n_vec = self.target_modules[index_conv * self.base_number].data.nelement()\n",
        "            k_size = self.target_modules[index_conv * self.base_number].data.size()\n",
        "\n",
        "            W = self.target_modules[index_conv * self.base_number].data.view(n_vec)\n",
        "\n",
        "            W_neg_mean = W.mean(dim=0, keepdim=True).neg().expand(n_vec)\n",
        "            W_std = W.std(dim=0, keepdim=True).expand(n_vec)\n",
        "            if self.base_number == 1:\n",
        "                B = W.add(W_neg_mean).sign().view(1, n_vec)\n",
        "            if self.base_number == 3:\n",
        "                t1 = W.add(W_neg_mean).add(W_std.mul(-1)).sign().view(1, n_vec)\n",
        "                t2 = W.add(W_neg_mean).sign().view(1, n_vec)\n",
        "                t3 = W.add(W_neg_mean).add(W_std).sign().view(1, n_vec)\n",
        "                B = torch.cat((t1, t2, t3))\n",
        "            # for base in range(self.base_number):\n",
        "            #     u_i=-1 + base * 2 / (self.base_number-1)\n",
        "            #     t=W.add(W_neg_mean).add(W_std.mul(u_i)).sign()\n",
        "            #     if base==0:\n",
        "            #         B=t.view(1,n_vec)\n",
        "            #     else:\n",
        "            #         B=torch.cat((B,t.view(1,n_vec)))\n",
        "            LRM = LinearRegression()\n",
        "            LRM.fit(B.t().cpu(), W.cpu())\n",
        "            # alpha = torch.from_numpy(LRM.coef_)\n",
        "            if platform.system() == \"Windows\":\n",
        "                alpha = torch.Tensor(LRM.coef_)\n",
        "            else:\n",
        "                alpha = torch.Tensor(LRM.coef_).cpu().cuda()\n",
        "\n",
        "            self.alphas[index_conv].copy_(alpha)\n",
        "            for base in range(self.base_number):\n",
        "                self.target_modules[index_conv * self.base_number + base].data.copy_(\n",
        "                    B[base].mul(alpha[base]).view(k_size))\n",
        "\n",
        "    def ABC_updateBinaryGradWeight(self):\n",
        "        # original version:\n",
        "        for index_conv in range(int(self.num_of_params / self.base_number)):\n",
        "            if self.base_number == 1:\n",
        "                pass\n",
        "            if self.base_number == 3:\n",
        "                # explanation of dW=dB*alpha^2:\n",
        "                # dB=d(L)/d(alpha*B)=1/alpha*d(L)/d(B)\n",
        "                alpha_dB1 = self.target_modules[index_conv * self.base_number].grad.data. \\\n",
        "                    mul(self.alphas[index_conv][0] * self.alphas[index_conv][0])\n",
        "                alpha_dB2 = self.target_modules[index_conv * self.base_number + 1].grad.data. \\\n",
        "                    mul(self.alphas[index_conv][1] * self.alphas[index_conv][1])\n",
        "                alpha_dB3 = self.target_modules[index_conv * self.base_number + 2].grad.data. \\\n",
        "                    mul(self.alphas[index_conv][2] * self.alphas[index_conv][2])\n",
        "\n",
        "                dW = alpha_dB1.add(alpha_dB2).add(alpha_dB3)\n",
        "                # attach STE to single base OR the sum of them?\n",
        "                W = self.target_modules[index_conv * self.base_number].data\n",
        "                dW[W.lt(-1)] = 0\n",
        "                dW[W.gt(1)] = 0\n",
        "                dW.mul(1e+9)\n",
        "                self.target_modules[index_conv * self.base_number].grad.data.copy_(dW)\n",
        "\n",
        "    binarizeConvParams = ABC_binarizeConvParams\n",
        "    updateBinaryGradWeight = ABC_updateBinaryGradWeight\n",
        "\n",
        "    def restore(self):\n",
        "        for index in range(int(self.num_of_params / self.base_number)):\n",
        "            self.target_modules[index * self.base_number].data.copy_(self.saved_params[index * self.base_number])\n",
        "\n",
        "    \n",
        "    #"
      ],
      "metadata": {
        "id": "i_U7Z9Rq1b8p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tHu37cjC1MV"
      },
      "source": [
        "#### Calculating alphas\n",
        "Now, we can calculate alphas using the *binary filters* and *convolution filters* by minimizing the *squared difference*\n",
        "$\\|\\mathbf{W}-\\mathbf{B}\\alpha\\|^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sJ5K6Y_C1MY"
      },
      "source": [
        "### Creating ApproxConv using the binary filters\n",
        "$\\mathbf{O}=\\sum\\limits_{m=1}^M\\alpha_m\\operatorname{Conv}(\\mathbf{B}_m, \\mathbf{A})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZFgsz8-C1MZ"
      },
      "source": [
        "As in mentioned in the paper, it is better to train the network first with simple Convolution networks and then convert the filters into the binary filters, allowing original filters to be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmYitfJGC1Mb"
      },
      "source": [
        "### Multiple binary activations and bitwise convolution\n",
        "Now, convolution can be achieved using just the summation operations by using the ApproxConv layers. But the paper suggests something even better. We can even bypass the summation through bitwise operations only, if the input to the convolution layer is also binarized.\n",
        "For that the authors suggests that an input can be binarized (creating multiple inputs) by shifting the inputs and binarizing them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZyWBdhC1Mc"
      },
      "source": [
        "First, the input is clipped between 0. and 1. using multiple shift parameters $\\nu$, learnable by the network  \n",
        "$\\operatorname{h_{\\nu}}(x)=\\operatorname{clip}(x + \\nu, 0, 1)$  \n",
        "  \n",
        "Then using the following function it is binarized  \n",
        "$\\operatorname{H_{\\nu}}(\\mathbf{R})=2\\mathbb{I}_{\\operatorname{h_{\\nu}}(\\mathbf{R})\\geq0.5}-1$\n",
        "\n",
        "The above function can be implemented as  \n",
        "$\\operatorname{H_{\\nu}}(\\mathbf{R})=\\operatorname{sign}(\\mathbf{R} - 0.5)$\n",
        "\n",
        "Now, after calculating the **ApproxConv** over each separated input, their weighted summation can be taken using trainable paramters $\\beta s$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOPZFDLNC1Me"
      },
      "source": [
        "## Testing\n",
        "Let's just test our network using MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cidCwnAuNSs9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "31fb467e373041e6a3a0c68bc6dc4182",
            "582b5a7c39544f129763cc4be3a37e33",
            "11ab7f5e9970405bb32597029a506e99",
            "6259fe3404a042199a887acb0449d8f8",
            "7fd6f9343bd34175b04dc94fb95a8c3f",
            "529647bb4cb347168de8f8c187285da8",
            "7f891d4a5e9f41d4bb8d1d5a88ff5b5c",
            "811aa04ee6fa4c56a3002fea6786ceb0",
            "2fdb124403494b10afef399957de0e93",
            "f6da1aba2ebf40df96e651f3d5024215",
            "0820bde0a2954010b67c24af8e8bc0cc"
          ]
        },
        "outputId": "a325de2a-fca6-4f8e-be79-66232e4d5327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31fb467e373041e6a3a0c68bc6dc4182"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "AlexNet(\n",
            "  (features): DataParallel(\n",
            "    (module): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4))\n",
            "      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): ABCConv2d(\n",
            "        (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bases_conv2d_1): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (bases_conv2d_2): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (bases_conv2d_3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (6): ABCConv2d(\n",
            "        (bn): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bases_conv2d_1): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bases_conv2d_2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bases_conv2d_3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): ABCConv2d(\n",
            "        (bn): BatchNorm2d(384, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bases_conv2d_1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bases_conv2d_2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bases_conv2d_3): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (8): ABCConv2d(\n",
            "        (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bases_conv2d_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bases_conv2d_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bases_conv2d_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (9): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): ABCConv2d(\n",
            "      (bn): BatchNorm1d(9216, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bases_linear_1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "      (bases_linear_2): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "      (bases_linear_3): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ABCConv2d(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (bn): BatchNorm1d(4096, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bases_linear_1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (bases_linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (bases_linear_3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): BatchNorm1d(4096, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Learning rate: 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/196]\tTime 28.929 (28.929)\tData 13.284 (13.284)\tLoss 2.3029 (2.3029)\tPrec@1 13.281 (13.281)\tPrec@5 48.438 (48.438)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "import gc\n",
        "import platform\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
        "\n",
        "parser.add_argument('--data', metavar='DATA_PATH', default='./data/',\n",
        "                    help='path to imagenet data (default: ./data/)')\n",
        "\n",
        "parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',\n",
        "                    help='number of data loading workers (default: 8)')\n",
        "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
        "                    metavar='N', help='mini-batch size (default: 256)')\n",
        "parser.add_argument('--base_number', default=3, type=int,\n",
        "                    metavar='N', help='base_number (default: 3)')\n",
        "parser.add_argument('--epochs', default=5, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
        "                    help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.001, type=float,\n",
        "                    metavar='LR', help='initial learning rate')\n",
        "parser.add_argument('--momentum', default=0.90, type=float, metavar='M',\n",
        "                    help='momentum')\n",
        "parser.add_argument('--weight-decay', '--wd', default=1e-5, type=float,\n",
        "                    metavar='W', help='weight decay (default: 1e-5)')\n",
        "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
        "                    metavar='N', help='print frequency (default: 10)')\n",
        "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
        "                    help='path to latest checkpoint (default: none)')\n",
        "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
        "                    help='evaluate model on validation set')\n",
        "parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
        "                    default=False, help='use pre-trained model')\n",
        "parser.add_argument('--nocuda', dest='nocuda', action='store_true',\n",
        "                    help='running on no cuda')\n",
        "best_prec1 = 0\n",
        "\n",
        "# define global bin_op\n",
        "bin_op = None\n",
        "\n",
        "# define optimizer\n",
        "optimizer = None\n",
        "\n",
        "\n",
        "def main():\n",
        "    global args, best_prec1\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if platform.system() == \"Windows\":\n",
        "        args.nocuda = True\n",
        "    else:\n",
        "        args.nocuda = False\n",
        "\n",
        "    # create model\n",
        "    \n",
        "    model = alexnet(pretrained=args.pretrained, base_number=args.base_number)\n",
        "    input_size = 227\n",
        "    model.features = torch.nn.DataParallel(model.features)\n",
        "    if not args.nocuda:\n",
        "        # set the seed\n",
        "        torch.manual_seed(1)\n",
        "        torch.cuda.manual_seed(1)\n",
        "        model.cuda()\n",
        "        # define loss function (criterion) and optimizer\n",
        "        criterion = nn.CrossEntropyLoss().cuda()\n",
        "        # Set benchmark\n",
        "        cudnn.benchmark = True\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    global optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), args.lr,\n",
        "                                 weight_decay=args.weight_decay)\n",
        "    # random initialization\n",
        "    if not args.pretrained:\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                c = float(m.weight.data[0].nelement())\n",
        "                m.weight.data = m.weight.data.normal_(0, 1.0 / c)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data = m.weight.data.zero_().add(1.0)\n",
        "    else:\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data = m.weight.data.zero_().add(1.0)\n",
        "    # optionally resume from a checkpoint\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "            # original saved file with DataParallel\n",
        "            checkpoint = torch.load(args.resume)\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            best_prec1 = checkpoint['best_prec1']\n",
        "            print(checkpoint)\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(args.resume, checkpoint['epoch']))\n",
        "            del checkpoint\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
        "\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    traindir = os.path.join(args.data, 'ILSVRC2012_img_train')\n",
        "    valdir = os.path.join(args.data, 'ILSVRC2012_img_val')\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "    val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "    if not args.nocuda:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "            num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "            num_workers=args.workers, pin_memory=True)\n",
        "    else:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "            num_workers=args.workers)\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "            num_workers=args.workers)\n",
        "\n",
        "    print(model)\n",
        "\n",
        "    # define the binarization operator\n",
        "    global bin_op\n",
        "    bin_op =BinOp(model)\n",
        "\n",
        "    if args.evaluate:\n",
        "        validate(val_loader, model, criterion)\n",
        "        return\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'arch': args.arch,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_prec1': best_prec1,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }, is_best)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        if not args.nocuda:\n",
        "            target = target.cuda(non_blocking=True)\n",
        "            input_var = torch.autograd.Variable(input).cuda()\n",
        "        else:\n",
        "            input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "\n",
        "        # process the weights including binarization\n",
        "        bin_op.binarization()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
        "        losses.update(loss.data, input.size(0))\n",
        "        top1.update(prec1[0], input.size(0))\n",
        "        top5.update(prec5[0], input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # restore weights\n",
        "        bin_op.restore()\n",
        "        bin_op.updateBinaryGradWeight()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0 :\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "\n",
        "        # because the training process is too slow\n",
        "        if i % 100 == 99:\n",
        "            save_checkpoint({\n",
        "                'arch': args.arch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, False, filename=\"checkpoint_every_100_batches.pth.tar\")\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    bin_op.binarization()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "\n",
        "        if not args.nocuda:\n",
        "            target = target.cuda(non_blocking=True)\n",
        "            input_var = torch.autograd.Variable(input, volatile=True).cuda()\n",
        "            target_var = torch.autograd.Variable(target, volatile=True)\n",
        "        else:\n",
        "            input_var = torch.autograd.Variable(input, volatile=True)\n",
        "            target_var = torch.autograd.Variable(target, volatile=True)\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
        "        losses.update(loss.data, input.size(0))\n",
        "        top1.update(prec1[0], input.size(0))\n",
        "        top5.update(prec5[0], input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        \n",
        "        #if i % args.print_freq == 0 :\n",
        "        print('Test: [{0}/{1}]\\t'\n",
        "                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                i, len(val_loader), loss=losses,\n",
        "                top1=top1, top5=top5))\n",
        "    bin_op.restore()\n",
        "\n",
        "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
        "          .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 25 epochs\"\"\"\n",
        "    #lr = args.lr * (0.1 ** (epoch // 25))   \n",
        "    lr=0.2 \n",
        "    print('Learning rate:', lr)\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    # print(pred)\n",
        "    # print(target)\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "myfolder.py"
      ],
      "metadata": {
        "id": "BEVQ1V3kQ9rP"
      }
    }
  ]
}